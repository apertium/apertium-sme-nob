\newcommand{\comment}[1]{\textbf{TODO:~#1}}
%\renewcommand{\comment}[1]{} % uncomment for final version

% TODO:
%\usepackage[utf8x]{inputenc}

\newcommand{\href}[2]{{\tt #1}} % put hyperref in preamble? TODO

\newcommand{\sme}{{\tt sme}}
\newcommand{\nob}{{\tt nob}}
\newcommand{\smenob}{\sme$\rightarrow{}$\nob}
\newcommand{\nobsme}{\nob$\rightarrow{}$\sme}

\achapter{Evaluating a North Sami to Norwegian assimilation RBMT system}{N.N. and N.N.}

% testen:
% a. teste ut 10x10x10
% b. evaluere 

% skrivinga:
% - leite etter litteratur om evaluering av system for å forstå tekst
% - skrive rammetekst, lage dummy-tabellar for testresultat
% - så, når testinga er ferdig, fylle inn resultat, skrive ferdig, 

% disposisjon
% - intro: mål og plan for artikkelen
% - samisk, gisting, tidlegare litteratur
% - vår test, vår analyse
% - oppsummering og konklusjon, evaluering av kva som er potensiale for forbetring


  
    % \begin{abstract}
    We describe the development of a rule-based machine translation
    system from Northern Sámi to Norwegian Bokmål built on a
    combination of Free and Open Source (FOSS) resources: the Apertium
    platform and the Giellatekno HFST lexicon and Constraint Grammar
    disambiguator.
    \comment{more}
    We detail the integration of these and other resources in the
    system along with the construction of the lexical and structural
    transfer, and evaluate the translation quality in comparison with
    another system. Finally, some future work is suggested.
    % \end{abstract}




\section{Introduction}
This paper describes the development of a rule-based machine
translation system from Northern Sámi to Norwegian Bokmål, re-using
several existing free and open source resources. \comment{Roadmap}

\subsection{The Languages}
Northern Sámi (\sme{}) is a Finno-Ugric language spoken by between
15,000 and 25,000 people in the northern parts of Norway, Sweden and
Finland. Norwegian Bokmål (\nob{}) is a North Germanic language with
about 4.5 speakers, mostly in Norway. Northern Sámi is a highly
inflected, agglutinative language, whereas Norwegian morphology is
comparatively simple.

Most \sme{} speakers in Norway understand \nob{}, while most \nob{}
speakers do not understand \sme{}. The languages are completely
unrelated, and the linguistic distance is great, making it hard to
achieve high quality MT results. A \nobsme{} system would only be useful if the
quality were good enough that it could be used for text production
(post-editing). On the other hand, a \smenob{} gisting-quality system
can be useful for the large group of \nob{} speakers who don't
understand \sme{}. Thus we chose to focus on the \smenob{} direction
first.

We do not know of other machine translation (MT) systems between \sme
and any Indo-European language, although \citet{tyers2009dpm} describe
a prototype system between Northern Sámi and Lule Sámi.



\section{Design}
 \label{sec:design}

\subsection{The Apertium Pipeline}
This language pair is based on the Apertium MT
platform\citep{forcada2011afp}. Apertium provides a highly modular,
shallow-transfer pipeline MT engine, as well as data for language
pairs. Both the engine and the data for all language pairs (about 30
released pairs as of now) are licensed under the
GPL\footnote{\href{http://www.fsf.org/licensing/licenses/gpl.html}{http://www.fsf.org/licensing/licenses/gpl.html}}.

Apertium language pairs are defined by Unix pipelines, where the
typical pipeline consists of:

\begin{itemize}
\item deformatting (hiding format information or markup from the
  engine),
\item source language morphological analysis with a finite state
  transducer (FST),
\item disambiguation using a Hidden Markov Model (HMM) and/or
  Constraint Grammar (CG), 
\item lexical transfer (word-translation on the disambiguated source),
\item one and one or more levels of structural transfer
  (ie.~reordering and changes to morphological features), 
\item target-language generation with an FST
\item reformatting (letting format information be shown again)
\end{itemize}

Most Apertium language pairs use the lttoolbox FST package for
analysis and generation. The lttoolbox dictionaries are written in
XML, where one dictionary may be compiled both to an analyser and a
generator. The \smenob{} pair uses lttoolbox for \nob{}
generation, while the \sme{} analyser is written in the Xerox lexc/twol
formats\citep{beesley2003fsm}. We use the FOSS package Helsinki Finite
State Tools, HFST \citep{linden2011hfst} to compile and run the
analyser (see section \ref{sec:hfst}).

The morphological analysis gives us ambiguous output with no syntactic
information. For morphological (e.g.~part-of-speech) disambiguation,
syntactic annotation/disambiguation and lexical
selection\footnote{Similar to Word Sense Disambiguation, but
  restricted to senses that have differing translations.}, we use
Constraint Grammar \citep{karlsson1990cgf}\footnote{Using the FOSS
  package {\tt \small VISL CG-3},
  \href{http://beta.visl.sdu.dk/cg3.html}{http://beta.visl.sdu.dk/cg3.html}}.
Morphological disambiguation and syntactic annotation/disambiguation
are run as one CG module, the output of which is unambiguous both
morphologically (one analysis per form) and syntactically (each
form/analysis is annotated with one syntactic tag).

The first CG module is directly followed by a lexical selection CG
module\footnote{If the disambiguation rules leaves any ambiguity, that
  module is configured only print the first analysis. We may later
  train an HMM to get rid of leftover ambiguity, this would go between
  the two CG modules.}, which may add subscripts to lemmas in certain
contexts in order to select a different lexical translation. 

Lexical selection is followed by pretransfer (minor format changes in
preparation of transfer) and then a four-stage finite-state-based
chunking transfer. The first stage handles lexical transfer as well as
chunking based on patterns of morphological and syntactic tags
(further detail in section \ref{sec:structural-transfer}).

Output from the transfer module is fed to morphological generation
with the lttoolbox-based \nob{} generator. De-/reformatters applied to
the beginning and end of the pipeline let us preserve formatting of
various document types.

\subsection{HFST}
\label{sec:hfst}
One novel feature of apertium-sme-nob is the HFST-based analyser. HFST
makes it possible to compile lexicons and morphologies originally
written for the closed-source Xerox Finite State Tools using only free
and open source tools, and run them with Apertium-compatible output
formats. As with most Xerox-based analysers, the \sme lexicon and
morphology are written in \textbf{lexc} and compiled into an FST, onto
which \textbf{twol} rules are composed which define the
morphophonology. HFST analysers are slower at compiling and processing
than lttoolbox, but certain morphological phenomena, especially
non-concatenative phenomena (e.g. \sme consonant gradation) are
impossible---or at least very difficult---to describe in lttoolbox.
Since Northern Sámi is quite morphologically complex, a purely
lttoolbox-based analyser would be difficult to maintain.


\section{Development}
  \label{sec:development}

This section describes how the language pair was developed.
\subsection{Resources}
We re-used several FOSS resources in creating this language pair. The
\nob{} generator came from {\tt
  apertium-nn-nb}\citep{unhammer2009rfr}, while most of the \sme{}
resources came from the Divvun and Giellatekno Sámi language
technology projects \footnote{See
  \href{http://divvun.no}{http://divvun.no} and
  \href{http://giellatekno.uit.no}{http://giellatekno.uit.no}.},
including the lexicon/morphology and disambiguator/syntactic
annotation CG. These were continually updated as the ``upstream''
versions changed.

The lexical selection CG and the transfer rules were written from
scratch. The translation lexicon was originally based on various
word-lists from Giellatekno
\href{http://giellatekno.uit.no}{http://giellatekno.uit.no}
\comment{was Vuosttaš Digisánit used?}, but expanded throughout
development.
\subsection{Analysis and derivational morphology}
The morphological analyser from N.N.\comment{Giellatekno} was not
originally made with machine translation in mind, and we had to make
several modifications, from using the Apertium tag format, to
restricting derivational morphology and removing root forms without
translations. The modifications in the language pair are all done
automatically using scripts, so that we can keep the analyser
up-to-date with the upstream version.

The upstream analyser contains many lemmas that are not in our
translational dictionary. Since these often lead to transfer errors
that can affect the surrounding context, and can suppress the choice
of forms that \textit{have} translations, we ``trim'' the analyser
down to those forms which are in the translational dictionary. To do
this, we use a script which analyses the lexc source files with the
translational dictionary, and outputs only those entries which have
translational analyses. The untrimmed source files weigh in at about
3.5 MB, trimming this down to 2.6 MB is done by a script which runs in
<10 seconds.

The original analyser defines quite a lot of rules for derivational
processes. Allowing for derivational morphology expands the coverage
of the analyser without having to add new root forms (lexicalisation),
but also makes transfer much harder to deal with, as well as most of
the time giving very odd-sounding translations.

To give an example of the latter, `geafivuohta' is an
adjective$\rightarrow{}$noun derivation of `geafi', meaning `poor'.
Simply carrying over the information that this is an
adjective$\rightarrow{}$noun derivation into the target language
dictionary (if that dictionary also defined derivational processes)
could give us forms that sound like `poorness' or `poority' or
`poordom', whereas giving `geafivuohta' status as a real root form
would let us specify that `geafivuohta' should translate to `poverty'.
Without derivations, `geafivuohta' would either be lexicalised and
translated to `poverty', or not translated at all.

Derivations also create extra transfer complexity. A causative verb
derivation requires transfer rules that turn the causative verb into a
periphrastic construction (e.g. `let NP VERB'). If a derivation
changes the part-of-speech from verb to noun, we have to translate the
derivation into a certain verb form that looks right in a noun context
(e.g.~present tense of \nob verbs will most of the time look like an
actor noun)\footnote{The alternative would be to define, for each \sme
  verb, both a noun and a verb translation on the \nob side of the
  translational dictionary, but this takes away the whole point of
  increasing coverage without adding all the root forms.}. To make
this even more complex, even a lexicalised form might require a
part-of-speech change in the transfer lexicon if there is no word with
the same meaning and part-of-speech in the target language; in that
case, we also have to ensure transfer works for all possible
derivations of all possible defined part-of-speech changes.

However, since \smenob is meant for gisting, where an odd-sounding
translation is more useful than an untranslated word, and resources
for automatically expanding the bilingual dictionary are scarce, we
decided to allow certain derivations. We define legal derivations by
the use of additional twol rules which simply forbid analyses
containing certain tags or tag sequences. These twol rules are
composed onto the main analyser in Apertium, but not used upstream.

\subsection{Disambiguation}
The CG created by Giellatekno was usable in Apertium with only minor
changes to tag naming, requiring very little manual intervention to
keep up-to-date. However, we did add Apertium-only rules which remove
derivations and compound readings if there are lexicalised readings
available, since we want lexically specified translations to override
the guesswork done by derivation transfer.

\subsection{Lexical selection}
A lexical selection CG was created in order to select between
different possible translations that otherwise share the same
part-of-speech information. Currently it has only 102 rules covering
52 lemmas, mostly high-frequency ones, although 802 lemmas of the
transfer lexicon have at least one alternative translation.

\subsection{Lexical transfer}


\subsection{Structural transfer}
\label{sec:structural-transfer}

\subsection{Generation}

\section{Evaluation}
\label{sec:eval}
The naïve coverage\footnote{A form is counted as covered if it gets at
  least one analysis. It might have ambiguity which the analyser does
  not cover, thus `naïve'.} of the analyser is shown in table
\ref{table:cov} for legal text (laws), the \sme Wikipedia (wiki) and a
corpus of \sme news articles. All forms which pass through the
analyser, will also pass through the transfer lexicon, transfer rules
and generator, so this shows the coverage of the other dictionaries
(in the \smenob direction) as well. Since derivations are not
specified in the translational dictionary, we show coverage with and
without derivation-only analyses counted. The table also shows the
ambiguity rate (amount of analyses per known word) with and without
derivations counted.

\begin{table}
  \begin{center}
  \begin{tabular}{crrrrr}
   Corpus     & tokens   & coverage & ambig.rate  & coverage w/o deriv & ambig.rate w/o deriv \\
   laws       &  51706   & 94.68\%  & 2.65        & 86.02\%            & 2.32 \\
   wiki       & 19942    & 77.52\%  & 2.36        & 74.56\%            & 2.19 \\
   news       & 1020250  & 94.72\%  & 2.59        & 90.96\%            & 2.34 \\
  \end{tabular}
    \caption{Naïve coverage on several corpora.}
    \label{table:cov}
  \end{center}
\end{table}
In the rest of this section we evaluate the practical performance of
the system using several methods. First we do a word-error rate test,
which shows how well the system would perform in a
post-editing/dissemination setting, then a set of tests meant to find
out how well the system performs in a gisting/assimilation setting.
All tests were run on revision 37177 of \texttt{apertium-sme-nob}.


\subsection{Word Error Rate on Post-Edited text}
\label{sec:WER}
We did a Word Error Rate test on a short children's story
\footnote{\href{https://apertium.svn.sourceforge.net/svnroot/apertium/branches/xupaixkar/rasskaz}{https://apertium.svn.sourceforge.net/svnroot/apertium/branches/xupaixkar/rasskaz}}
and some paragraphs from a history web page
\footnote{\href{http://skuvla.info/skolehist/siri97-s.htm}{http://skuvla.info/skolehist/siri97-s.htm}}.
The results are shown in \ref{table:wer}\footnote{Our post-edits are
  available in the repository.}. The translator obviously struggles
with the more complex formulations in the history text, and has a long
way to go before being useful for post-editing.


\begin{table}
  \begin{center}
  \begin{tabular}{ccrrr}
   Text       & tokens & Unknown & WER  \\
   children's & 415     & 5      & 45.96\% \\
   history    & 435     & 28     & 60.32\%  \\
  \end{tabular}
    \caption{Word error rate on two short texts.}
    \label{table:wer}
  \end{center}
\end{table}


\subsection{Gisting eval}
  
In order to evaluate to what extent the system was able to convey the
meaning of the original, we arranged a test containing 3 parts. All
the test were based on sentences from a parallel corpus of
non-fiction, the corpus had not been used during development of the MT
system. The first test, a multiple choice test, presented 10 sentences
drawn from the corpus. For each sentence, the test person also got the
translation, and 3 alternative paraphrases. The second test was set up
as the first, but instead of the 3 alternatives the test presented an
open question related to the translation.

The third test showed a \sme source sentence, then the MT output of
that sentence, followed by the reference translation (5-15 words long)
where at least two of the nouns were removed. For each removed noun,
we instead showed a randomised, clickable list consisting of the
originally removed word, along with a random choice of other
nouns\footnote{Nouns were of the same length (+- 3 characters), pulled
  from the same 55128 word long legal text, had the same morphological
  features (gender, definiteness, number) and were never ambiguous
  with verbs.} and finally a "none seem to fit" choice. The users (who
did not understand \sme) were instructed to click what seemed to be
the removed word, using the MT as a guide.


\subsection{Error analysis}

\section{Discussion and outlook}

The lexical selection CG can be automatically expanded with rules
created by the Apertium package \texttt{apertium-lex-tools}
\citep{???}.

\section*{Acknowledgements}
Development was funded by \comment{what are they called now?}
Thanks to N.N.


\nocite{zubizarreta2009amt}
