\newcommand{\comment}[1]{\textbf{TODO:~#1}}
\renewcommand{\comment}[1]{} % uncomment for final version

% TODO:
%\usepackage[utf8x]{inputenc}

\newcommand{\href}[2]{{\tt #1}} % put hyperref in preamble? TODO

\newcommand{\sme}{{\tt sme}}
\newcommand{\nob}{{\tt nob}}
\newcommand{\smenob}{\sme$\rightarrow{}$\nob}
\newcommand{\nobsme}{\nob$\rightarrow{}$\sme}

\achapter{Evaluating North S\'{a}mi to Norwegian assimilation RBMT}{N.N. and N.N.}

% testen:
% a. teste ut 10x10x10
% b. evaluere 

% skrivinga:
% - leite etter litteratur om evaluering av system for å forstå tekst
% - skrive rammetekst, lage dummy-tabellar for testresultat
% - så, når testinga er ferdig, fylle inn resultat, skrive ferdig, 

% disposisjon
% - intro: mål og plan for artikkelen
% - samisk, gisting, tidlegare litteratur
% - vår test, vår analyse
% - oppsummering og konklusjon, evaluering av kva som er potensiale for forbetring


  
    % \begin{abstract}
We describe the development and evaluation of a rule-based machine
translation assimilation system from North S\'{a}mi to Norwegian Bokm{\aa}l,
built on a combination of Free and Open Source (FOSS) resources: the
Apertium platform and the Giellatekno HFST lexicon and Constraint
Grammar disambiguator. 
%Linda: whats the difference between free and OS ?
%Kevin: http://en.wikipedia.org/wiki/Free_and_open_source_software
%FOSS is a fairly established term

We detail the integration of these and other resources in the system
along with the construction of the lexical and structural transfer,
and evaluate the translation quality using various methods, focusing
on evaluating the users' \textit{comprehension} of the text. Finally,
some future work is suggested.
% Linda: the abstract sounds a bit boring, we do this, then that, 
% then this.. maybe you can put an exciting first sentence including 
% 000the special thing about the work
    % \end{abstract}




\section{Introduction} % and roadmap
This paper describes and evaluates a rule-based machine translation
(MT) system from North S\'{a}mi to Norwegian Bokm{\aa}l, re-using
several existing free and open source resources. 

We begin with an introduction to the languages and the technology
used, followed by a description of how the system was developed. Then
we evaluate the system with respect to \textit{assimilation}: MT with
the purpose of letting users understand a text written in a language
foreign to them (as opposed to \textit{dissemination}, where the
purpose is MT for post-editing). Finally we discuss the results and
ideas for improvements.

\subsection{The Languages}
North S\'{a}mi (\sme{}) is a Finno-Ugric language spoken by between 
15,000 and 25,000 people in the northern parts of Norway, Sweden and
Finland. Norwegian Bokm{\aa}l (\nob{}) is a North Germanic language
with about 4.5 million speakers, mostly in Norway. North S\'{a}mi %mostly in Norway and where else?
is a highly inflected, agglutinative language, whereas Norwegian
morphology is comparatively simple.

Most \sme{} speakers in Norway understand \nob{}, while most \nob{}
speakers do not understand \sme{}. The languages are completely
unrelated, and the linguistic distance is great, making it hard to
achieve high quality MT results. A \nobsme{} system would only be useful if the
quality were good enough that it could be used for text production
(post-editing). On the other hand, a \smenob{} gisting-quality system
can be useful for the large group of \nob{} speakers who don't
understand \sme{}. Thus we chose to focus on the \smenob{} direction
first.

We do not know of other machine translation (MT) systems between \sme{}
and any Indo-European language, although \citet{tyers2009dpm} describe %Linda: thanks for the citation ;)
a prototype system between North S\'{a}mi and Lule S\'{a}mi.



\section{Design}
 \label{sec:design}

\subsection{The Apertium Pipeline}
This language pair is based on the Apertium MT
platform \citep{forcada2011afp,zubizarreta2009amt}. Apertium provides a
highly modular, shallow-transfer pipeline MT engine, as well as data
for language pairs. Both, the engine and the data for all language
pairs (about 30 released pairs as of now) are licensed under the
GPL\footnote{\href{http://www.fsf.org/licensing/licenses/gpl.html}{http://www.fsf.org/licensing/licenses/gpl.html}}.

Apertium language pairs are defined by Unix pipelines, where the
typical pipeline consists of:

\begin{itemize}
\item deformatting (hiding formatting/markup from the engine),
\item source morphological analysis with a finite state transducer
  (FST),
\item disambiguation using a Hidden Markov Model (HMM) and/or
  Constraint Grammar (CG), 
\item lexical transfer (word-translation on the disambiguated source),
\item one or more levels of finite-state based structural transfer
  (reordering, and changes to morphological features),
\item target-language generation with an FST
\item reformatting (letting format information be shown again)
\end{itemize}

Most Apertium language pairs use the lttoolbox FST package for
analysis and generation. The lttoolbox dictionaries are written in
XML, where one dictionary may be compiled both to an analyser and a
generator. The \smenob{} pair uses lttoolbox for \nob{} generation and
the transfer lexicon, while the \sme{} analyser is written in the
Xerox lexc/twol formats~\citep{beesley2003fsm}. Both systems allow
generalising over classes using paradigms/continuation lexicons, but
differ in other features. We use the FOSS package Helsinki Finite
State Tools, HFST \citep{linden2011hfst} to compile and run the
analyser (see section \ref{sec:hfst}).

The morphological analysis gives us ambiguous output with no syntactic
information. For morphological (e.g.~part-of-speech) disambiguation,
syntactic annotation/disambiguation and lexical
selection\footnote{Like Word Sense Disambiguation, restricted to
  senses that have differing translations.}, we use Constraint Grammar
\citep{karlsson1990cgf}\footnote{Using the FOSS package {\tt \small
    VISL CG-3},
  \href{http://beta.visl.sdu.dk/cg3.html}{http://beta.visl.sdu.dk/cg3.html}}.
Morphological disambiguation and syntax
are run as one CG module, the output of which is unambiguous both
morphologically (one analysis per form) and syntactically (each
form/analysis is annotated with one syntactic tag).

The first CG module is directly followed by a lexical selection CG
module\footnote{If the disambiguation rules leaves any ambiguity, that
  CG only prints the first analysis. We may later train an HMM to get
  rid of leftover ambiguity, this would go between the two CG
  modules.}, which may add subscripts to lemmas in certain contexts in
order to select a different lexical translation.

Lexical selection is followed by pretransfer (minor format changes in
preparation of transfer) and then a four-stage chunking transfer. The
first stage handles lexical transfer as well as chunking based on
patterns of morphological and syntactic tags (further detail in
section \ref{sec:structural-transfer}).

Output from the transfer module is fed to morphological generation
with the lttoolbox-based \nob{} generator. De-/reformatters applied to
the beginning and end of the pipeline let us preserve formatting of
various document types.

\subsection{HFST}
\label{sec:hfst}
One novel feature of apertium-sme-nob is the HFST-based analyser. HFST
makes it possible to compile lexicons and morphologies originally
written for the closed-source Xerox Finite State Tools using only free
and open source tools, and run them with Apertium-compatible output
formats. As with most Xerox-based analysers, the \sme{} lexicon and
morphology are written in \textbf{lexc} and compiled into an FST, onto
which \textbf{twol} rules are composed which define the
morphophonology. HFST analysers are slower at compiling and processing
than lttoolbox, but certain morphological phenomena, especially
non-concatenative phenomena (e.g. \sme{} consonant gradation) are
impossible---or at least very difficult---to describe in lttoolbox.
Since North S\'{a}mi is quite morphologically complex, a purely
lttoolbox-based analyser would be difficult to maintain.


\section{Development}
  \label{sec:development}

This section describes how the language pair was developed.
\subsection{Resources}
We re-used several FOSS resources in creating this language pair. The
\nob{} generator came from {\tt
  apertium-nn-nb} \citep{unhammer2009rfr}, while most of the \sme{}
resources came from the Divvun and Giellatekno S\'{a}mi language
technology projects \footnote{See
  \href{http://divvun.no}{http://divvun.no} and
  \href{http://giellatekno.uit.no}{http://giellatekno.uit.no}.},
including the lexicon/morphology and disambiguator/syntactic
annotation CG. These were continually updated as the ``upstream''
versions changed.

The lexical selection CG and the transfer rules were written from
scratch. The translation lexicon was originally based on various
word-lists from Giellatekno
but expanded throughout
development.
\subsection{Analysis and derivational morphology}
The morphological analyser from N.N.\comment{Giellatekno} was not
originally made with machine translation in mind, and we had to make
several modifications, from using the Apertium tag format, to
restricting derivational morphology and removing root forms without
translations. The modifications in the language pair are all done
automatically using scripts, so that we can keep the analyser
up-to-date with the upstream version.

The upstream analyser contains many lemmas that are not in our
translational dictionary. Since these often lead to transfer errors
that can affect the surrounding context, and can suppress the choice
of forms that \textit{have} translations, we ``trim'' the analyser
down to those forms which are in the translational dictionary. To do
this, we use a script which analyses the lexc source files with the
translational dictionary, and outputs only those entries which have
translational analyses. The untrimmed source files weigh in at about
3.5 MB, trimming this down to 2.6 MB is done by a script which runs in
<10 seconds.

The original analyser defines quite a lot of rules for derivational
processes. Allowing for derivational morphology expands the coverage
of the analyser without having to add new root forms (lexicalisation),
but also makes transfer much harder to deal with, as well as most of
the time giving very odd-sounding translations.

To give an example of the latter, `geafivuohta' is an 
adjective$\rightarrow{}$noun derivation of `geafi', meaning `poor'.
Simply carrying over the information that this is an
adjective$\rightarrow{}$noun derivation into the target language
dictionary (if that dictionary also defined derivational processes)
could give us forms that sound like `poorness' or `poority' or
`poordom', whereas giving `geafivuohta' status as a real root form
would let us specify that `geafivuohta' should translate to `poverty'.
Without derivations, `geafivuohta' would either be lexicalised and
translated to `poverty', or not translated at all.

Derivations also create extra transfer complexity. A causative verb
derivation requires transfer rules that turn the causative verb into a
periphrastic construction (e.g. `let NP VERB'). If a derivation
changes the part-of-speech from verb to noun, we have to translate the
derivation into a certain verb form that looks right in a noun context
(e.g.~present tense of \nob verbs will most of the time look like an
actor noun)\footnote{The alternative would be to define, for each \sme{}
  verb, both a noun and a verb translation on the \nob side of the
  translational dictionary, but this takes away the whole point of
  increasing coverage without adding all the root forms.}. To make
this even more complex, even a lexicalised form might require a
part-of-speech change in the transfer lexicon if there is no word with
the same meaning and part-of-speech in the target language; in that
case, we also have to ensure transfer works for all possible
derivations of all possible defined part-of-speech changes.

However, since \smenob{} is meant for gisting, where an odd-sounding
translation is more useful than an untranslated word, and resources
for automatically expanding the bilingual dictionary are scarce, we
decided to allow certain derivations. We define legal derivations by
the use of additional twol rules which simply forbid analyses
containing certain tags or tag sequences. These twol rules are
composed onto the main analyser in Apertium, but not used upstream.

\subsection{Disambiguation}
The CG created by Giellatekno was usable in Apertium with only minor
changes to tag naming, requiring very little manual intervention to
keep up-to-date. However, we did add Apertium-only rules which remove
derivations and compound readings if there are lexicalised readings
available, since we want lexically specified translations to override
the guesswork done by derivation transfer. Certain discrepancies in
the tag set still exist though, which may affect disambiguation
quality.

\subsection{Lexical selection}
A lexical selection CG was created in order to select between
different possible translations that otherwise share the same
part-of-speech information. Currently it has only 102 rules covering
52 lemmas, mostly high-frequency ones, although 802 lemmas of the
transfer lexicon have at least one alternative translation. %Linda: that's a bit short

\subsection{Lexical transfer}
The open classes of the transfer lexicon were initiated with entries
from the 9900 lemma dictionary Vuosttaš Digis\'{a}nit\footnote{GPL and CC,
  see
  \href{http://giellatekno.uit.no/words/dicts/index.eng.html}{http://giellatekno.uit.no/words/dicts/index.eng.html}.},
although many multiword translations had to be removed or simplified;
later on, entries were mostly added manually. Not including lexical
selection alternatives, there are currently about 3300 verbs, 1400
adjectives and 14000 nouns in the transfer lexicon.

Unlike with most Apertium language pairs, we did not make an attempt
to change the tag set in the analyser to conform with the Apertium
standard (apart from minor format differences). The change from e.g.
\texttt{<N><Prop>} (proper noun) to \texttt{<np>} or \texttt{<Sg1>} to
\texttt{<sg><p1>} happens in the transfer lexicon, mostly using a
\textbf{paradigm} definition to generalise over changes for each part
of speech. Part of the derivation handling also happens here, e.g. the
most passive derivations turn into plain passive forms, while verbs
derived into actor-nouns are transferred to present tense verbs.

We also add tags, which are used only as a signal to transfer, which
are removed before generation. The causative derivation of a form gets
a tag which makes transfer form a periphrastic `let'-construction, but
we also add the same tag to all forms if the root itself is a
lexicalised causative (similarly with lexicalised passives). Verbs are
tagged according to the most likely animacy of the agent.

\subsection{Structural transfer}
\label{sec:structural-transfer}
Structural transfer is divided into four stages, with different
responsibilities:

\begin{enumerate}

\item Chunking, 63 rules: noun phrases turn into larger chunks,
  prepositions are output based on case information, verb auxiliaries
  and adverbs are output based on verb modality, voice and derivation
  tags.

\item Interchunk 1, 26 rules: simple anaphora resolution (most recent
  subject gender), merging coordinated noun phrase chunks, moving
  postpositions before noun phrases.

\item Interchunk 2, 39 rules: major word order changes, inserting
  dropped pronouns, inserting adverbs to indicate verb modality,
  correcting noun phrase definiteness using verb information.

\item Postchunk, 29 rules: inserting articles/determiners and the
  infinitive marker, tag cleanup in preparation of generation.
\end{enumerate}

A lot of work went into structural transfer compared to what is
typical of Apertium language pairs between more related languages.

\subsection{Generation}
The generator was re-used from the language pair
\texttt{apertium-nn-nb} with very few changes: We added some root
forms to the lexicon, and added a tag to distinguish synthetic from analytic
adjectives (a change which might later be useful in improving
\texttt{apertium-nn-nb}).

\section{Evaluation}
\label{sec:eval}
The na\"{i}ve coverage\footnote{A form is counted as covered if it
  gets at least one analysis. It might have ambiguity which the
  analyser does not cover, thus `na\"{i}ve'.} of the analyser is shown
in table \ref{table:cov} for legal text (laws), the \sme{} Wikipedia
(wiki) and a corpus of \sme{} news articles. All forms that pass
through the analyser, will also pass through the transfer lexicon,
transfer rules and generator, so this shows the coverage of the other
dictionaries (in the \smenob{} direction) as well. Since derivations
are not specified in the translational dictionary, we show coverage
with and without derivation-only analyses counted. The table also
shows the ambiguity rate (amount of analyses per known word) with and
without derivations counted. \footnote{Currently, the ambiguity rate
  is reduced to about 1.04 by the constraint grammar disambiguator if
  we turn off the switch to choose the first analysis given remaining
  ambiguity.}


The Wikipedia corpus seems to have very low coverage, but looking over
the unknown words, it seems that many of them are in Finnish, English
or Norwegian (the rest are mostly proper names). The S\'{a}mi Wikipedia
is also written by non-natives, 12.5\% of its 101000 words are not recognised
even by Giellatekno's non-normative analyser, as opposed to only 3.5 \% for
a larger, 6.1m reference corpus. The lower coverage for Wikipedia is thus to
be expected.


\begin{table}
  \begin{center}
  \begin{tabular}{crrrrr}
   Corpus     & tokens   & coverage & ambig.      & coverage   & ambig.rate  \\
              &          &          & rate        & w/o deriv  & w/o deriv \\
   laws       &  51706   & 94.68\%  & 2.65        & 86.02\%    & 2.32 \\
   wiki       & 19942    & 77.52\%  & 2.36        & 74.56\%    & 2.19 \\
   news       & 1020250  & 94.72\%  & 2.59        & 90.96\%    & 2.34 \\
  \end{tabular}
    \caption{Na\"{i}ve coverage on several corpora.}
    \label{table:cov}
  \end{center}
\end{table}
In the rest of this section we evaluate the practical performance of
the system using several methods. First we do a word-error rate test,
which shows how well the system would perform in a
post-editing/dissemination setting, then a set of tests meant to find
out how well the system performs in a gisting/assimilation setting.
All tests were run on revision 37177 of \texttt{apertium-sme-nob}.



\subsection{Word Error Rate on Post-Edited text}
\label{sec:WER}
We did a Word Error Rate test on a short children's story
\footnote{\href{https://apertium.svn.sourceforge.net/svnroot/apertium/branches/xupaixkar/rasskaz}{https://apertium.svn.sourceforge.net/svnroot/apertium/branches/xupaixkar/rasskaz}}
and some paragraphs from a history web page
\footnote{\href{http://skuvla.info/skolehist/siri97-s.htm}{http://skuvla.info/skolehist/siri97-s.htm}}.
The results are shown in \ref{table:wer}\footnote{Our post-edits are
  available from our public repository.}. The translator obviously
struggles with the more complex formulations in the history text, and
has a long way to go before being useful for post-editing.


\begin{table}
  \begin{center}
  \begin{tabular}{ccrrr}
   Text       & tokens & Unknown & WER  \\
   children's & 415     & 5      & 45.96\% \\
   history    & 435     & 28     & 60.32\%  \\
  \end{tabular}
    \caption{Word error rate on two short texts.}
    \label{table:wer}
  \end{center}
\end{table}


\subsection{Gisting evaluation}
  
In order to evaluate to what extent the system was able to convey the
meaning of the original, we arranged a test containing 3 parts. All
the test were based on sentences from a parallel corpus of
non-fiction, the corpus had not been used during development of the MT
system. The first test, a multiple choice test, presented 10 sentences
drawn from the corpus. For each sentence, the test person also got the
translation, and 3 alternative paraphrases. The second test was set up
as the first, but instead of the 3 alternatives the test presented an
open question related to the translation. For both test sets the 
questions and paraphrases were prepared before the sentences were 
translated by the system, in order not to be influenced by the 
translated output.

The third test showed a \sme{} source sentence, then the MT output of
that sentence, followed by the reference translation (5-15 words long)
where at least two of the nouns were removed. For each removed noun,
we instead showed a randomised, clickable list consisting of the
originally removed word, along with a random choice of other
nouns\footnote{Nouns were of the same length (+- 3 characters), pulled
  from the same 55128 word long legal text, had the same morphological
  features (gender, definiteness, number) and were never ambiguous
  with verbs. This questionnaire generator is available from our
  public repository and should be usable with other translators.} and
finally a "none seem to fit" choice. The users (who did not understand
\sme{}) were instructed to click what seemed to be the removed word,
using the MT as a guide. Ten consecutive sentences from the same piece
of text were shown one at a time.

\begin{table}[htdp]
\caption{Results of gisting evaluation, 3 different tests}
\begin{center}
\begin{tabular}{lccc}
Type & Multiple & Fill-in & Random \\
Result & 77 \% & 41 \% & 75 \% \\
Number of test subjects & 10 & 14  & 10  \\
\end{tabular}
\end{center}
\label{eval}
\end{table}%



The results from the multiple choice and random word tests correlate
with each other, whereas the fill-in test shows much worse results.
The reason for that is that the fill-in test was more vulnerable to
holes in the MT output. Four of the 10 test sentences got no or only
one correct answers. What made these sentences so hard was that the
system failed to translate the key word in the sentence. Sentence
(\ref{a}) gives an example.
%Linda: hard? hard to understand or maybe difficult?

\enumsentence[(a)]{{Mun muitt\'{a}n ahte Lillemor Isaksen, geas lei gymn\'{a}sa, measta ii fidnen oahpaheaddjibarggu go son lei s\'{a}megielat.} \\ 
{I remember that Lillemor Isaksen, who had secondary high school, almost did not get any work as a teacher, since she was \textbf{saami-speaking}}}

\enumsentence[(b)]{{Jeg husker at Lillemor Isaksen, som hadde *gymn\'{a}sa, nesten ikke han f{\aa}tt l{\ae}rerarbeidet da han var samisker.} \\
{I remember that Lillemor Isaksen, who had secondary high school, almost did not get any work as a teacher, since she was \textbf{a saamier}.}}

%Linda: maybe glossing is a good idea? with \exg. dont remember the package
%Kevin: eh...they require this hellish special latex system, so glosses
%might get low priority :/

The word $samisker$ (here interpreted as a agent noun, on a par with
$snekker$ ($carpenter$) does not exist, and is interpreted by 9 of the
informants as "a Saami", instead of the correct "a Saami speaker".
Here, the missing translation of the key word blocked a proper
understanding of the sentence, despite the rest of the sentence being
translated correctly. The loan word \textit{gymn\'{a}sa} was not
recognised, but understandable as it is a loan from Norwegian
($gymnas$).



\subsection{Error analysis}

The na\"{i}ve lexical coverage of the test sentences was good, 96.7\%, 
as compared to the coverage measured on our news corpus (91\%). With
an average sentence length of 14.5 words (as in our test set), the coverage
implies one lexical omission in every second sentence. For
some words, our analysis didn't include all likely readings (e.g.
\textit{muhte} should be ambiguous between a subjunction and a verb,
we only had the verb). In other cases, short (but non-compositional)
idioms were treated as individual words. A lot of anaphora get the
wrong gender, but it's hard to tell how badly this affects
comprehension.

For 3 of the 10 fill-in test
sentences the key word (the one


In the WER tests, we see some errors that are due to our translator
over-specifying, e.g. using ``the two'' as a subject for dual verbs
where ``they'' might be more natural. But for a gisting translator,
over-specific translation is a feature, not a bug.

Bad disambiguation is often at fault. The ambiguity rate after the
rules have run is quite low, but we seem to get many erroneous
readings, especially for high-frequency function words (e.g.
\textit{maid}, \textbf{also/what/that}). It is also obvious that we
need more lexical selection rules; some times the translations simply
sound non-fluent, but in other cases (e.g. \textit{lohkat},
\textbf{say/count}), the meaning is altered or lost.

The more complex the text, the more we see problems relating to
structural transfer; some times we simply don't catch large enough NP
chunks (since we only match fixed-length patterns, they turn into two
chunks instead of one), Derivations that work fine by themselves, can
give problems when part of complex NP's.



%% plan:
%% look at output, figure out what went wrong and what next
%% thus , bring evaluation beyond what hitchhikersguide characterised as
%% 42

So, 



\section{Discussion and outlook}
Currently, the results of the gisting evaluation indicate that the MT
output provides some help for non-Sámi speakers in understanding
North Sámi, but as the results of the fill-in sentence test showed,
users miss important points from isolated sentences at least.

The naïve coverage is very good on paper, even disregarding
derivations, but on the other hand, with a system meant for gisting,
one missing word can take away any chance of understanding the
sentence.

Lexical selection could be better. There is an experimental Apertium
package \texttt{apertium-lex-tools} that we plan to use to
automatically create more lexical selection rules. Disambiguation
might be improved by training an HMM to run after the rule-based
disambiguator, although the ambiguity rate is already reduced to 1.04
by the rules that are there \footnote{an HMM after CG would not help
  with the analyses that are erroneously removed, only those that are
  erroneously left ambiguous}. 

\section*{Acknowledgements}
Development was funded by the Norwegian Ministry of Government
Administration, Reform and Church Affairs. Thanks to N.N., N.N. and
N.N. who also contributed much to the development of this language
pair.


